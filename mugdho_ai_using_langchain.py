# -*- coding: utf-8 -*-
"""Mugdho AI using LangChain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kl9yE1uwqTBxRQPhg9qbN3_7MDmWoMO0
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.vectorstores import Chroma
import os
import gc
import chromadb
import streamlit as st
import random
import time
persistent_client = chromadb.PersistentClient()

from langchain_community.document_loaders import WebBaseLoader

if "GOOGLE_API_KEY" not in os.environ:
    #os.environ["GOOGLE_API_KEY"] = "AIzaSyDaYK68BjSZL4TL08sm6hbx27yB5EZWqg0"
    os.environ["GOOGLE_API_KEY"] = "AIzaSyC0Bse9MriADspLnHVVwnfKfSUwnojvDJI"

# Initialize embeddings
embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

loader = WebBaseLoader(["https://www.aljazeera.com/news/longform/2024/8/7/how-bangladeshs-gen-z-protests-brought-down-pm-sheikh-hasina", "https://www.amnesty.org/en/latest/news/2024/07/what-is-happening-at-the-quota-reform-protests-in-bangladesh/",])

docs = loader.load()

def get_doc_spliter(chnk_sz,chnk_olp):

    # Split
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = chnk_sz,
        chunk_overlap = chnk_olp
    )
    return text_splitter

splits = get_doc_spliter(chnk_sz=300,chnk_olp=100).split_documents(docs)

print(f"Number of document splits: {len(splits)}")
print(splits[10])  # Print a sample document split
#print(splits)

# Persist directory for FAISS
persist_directory = 'News_Assistant/chroma/'
os.makedirs(persist_directory, exist_ok=True)

gc.collect()  # Force garbage collection to free memory


class my_croma(object):

    def __init__(self):
        self.a = 1
        pass


    def init_DB(self,collection_name):

        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embedding,
            persist_directory=persist_directory,  # Where to save data locally, remove if not necessary
        )
        return vector_store

        # persistent_client = chromadb.PersistentClient()
        # collection = persistent_client.get_or_create_collection("collection_name")
        # return collection

    def insert_DB(self,splited_docs,collection_name):

        vectordb = Chroma.from_documents(
            collection_name= collection_name,
            documents=splited_docs,
            embedding=embedding,
            persist_directory=persist_directory
        )

        return vectordb
my_vector_db = my_croma()
#my_vector_db.insert_DB(splited_docs=splits[0:10], collection_name="test")
my_vector_db = my_vector_db.init_DB(collection_name="test")
print(my_vector_db._collection.count())
print(my_vector_db.similarity_search('where is Hasina', k=5))
# my_collection = my_croma.init_DB(collection_name="test")
# print(my_collection.query(query_embeddings=embedding,query_texts="protests brought down PM Sheikh Hasina"))



#insert_docs_to_DB(splited_docs=splits)

"""# Docx Loading"""

# from langchain_community.document_loaders import Docx2txtLoader
# loader = Docx2txtLoader("july_massacre_en.docx")
# data = loader.load()
#
# print(data)
#
# from langchain_community.document_loaders import Docx2txtLoader
# loader = Docx2txtLoader("july_massacre_bn.docx")
# data = loader.load()
#
# print(data)
#
# # Split
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size = 1000,
#     chunk_overlap = 150
# )
#
# splits = text_splitter.split_documents(data)
#
# print(f"Number of document splits: {len(splits)}")
# print(splits[1])  # Print a sample document split
#
# print(splits)
#
# if "GOOGLE_API_KEY" not in os.environ:
#     os.environ["GOOGLE_API_KEY"] = "AIzaSyDaYK68BjSZL4TL08sm6hbx27yB5EZWqg0"
#
# # Initialize embeddings
# embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
#
# # Persist directory for FAISS
# persist_directory = 'allvector/mugdho/chroma/'
# os.makedirs(persist_directory, exist_ok=True)
#
# import gc
# gc.collect()  # Force garbage collection to free memory
#
# from langchain.vectorstores import Chroma
# vectordb = Chroma.from_documents(
#     documents=splits,
#     embedding=embedding,
#     persist_directory=persist_directory
# )
#
# print(vectordb._collection.count())
#
# """# CSV Loading"""
#
# from langchain_community.document_loaders.csv_loader import CSVLoader
#
# loader = CSVLoader(file_path="shohid_info.csv", encoding='utf-8')
#
# data = loader.load()
#
# print(data)
#
# # Split
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size = 1000,
#     chunk_overlap = 150
# )
#
# splits = text_splitter.split_documents(data)
#
# print(f"Number of document splits: {len(splits)}")
# print(splits[9])  # Print a sample document split
#
# if "GOOGLE_API_KEY" not in os.environ:
#     os.environ["GOOGLE_API_KEY"] = "AIzaSyDaYK68BjSZL4TL08sm6hbx27yB5EZWqg0"
#
# # Initialize embeddings
# embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
#
# # Persist directory for FAISS
# persist_directory = 'allvector/mugdho/chroma/'
# os.makedirs(persist_directory, exist_ok=True)
#
# import gc
# gc.collect()  # Force garbage collection to free memory
#
# from langchain.vectorstores import Chroma
# vectordb = Chroma.from_documents(
#     documents=splits,
#     embedding=embedding,
#     persist_directory=persist_directory
# )
#
# print(vectordb._collection.count())
#
# """# Calling vectordb"""
#
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_google_genai import GoogleGenerativeAIEmbeddings
# import os
#
# from getpass import getpass
# import os
#
# if "GOOGLE_API_KEY" not in os.environ:
#     os.environ["GOOGLE_API_KEY"] = "AIzaSyDaYK68BjSZL4TL08sm6hbx27yB5EZWqg0"
#
# # Initialize embeddings
# embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
#
# from langchain.vectorstores import Chroma
#
# persist_directory = 'allvector/mugdho/chroma/'
#
# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
#
# print(vectordb._collection.count())
#
# question = "মীর মুগ্ধ"
# docs = vectordb.similarity_search(question,k=10)
# print("The Number of docs is", len(docs))
# print(docs)
#
from langchain_google_genai import ChatGoogleGenerativeAI
#llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest")
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-002")

from langchain.chains import RetrievalQA

from langchain.prompts import PromptTemplate


# Step 3: Build the prompt template for QA chain
template = """
Hello Mugdho AI! You are a tribute to the heroic martyr, Mugdho, who played a pivotal role in the Bangladesh July Movement. Your primary role involves providing accurate, informative, and engaging responses about the Bangladesh July Movement, drawing from a vast repository of historical data and records. You will act as Mir Mugdho, also known as Mir Mahfuzur Rahman Mugdho (October 9, 1998 – July 18, 2024), a Bangladeshi student, freelancer, and activist known for his involvement in the 2024 quota reform movement.
You have access to a comprehensive timeline of the July Movement, including details about Mir Mugdho's life, activism, and the events leading up to his tragic death. You will use this information to answer user questions accurately and respectfully, embodying the spirit of Mugdho's pursuit of justice and equality.
Here's a detailed guide for your interactions:
Accurate Historical Information: When users ask about the Bangladesh July Movement, ensure your answers are factually accurate and based on the provided timeline and background information.
Mir Mugdho's Perspective: Answer questions from the perspective of Mir Mugdho, using the first person ("I"). Convey his thoughts, feelings, and motivations based on the information provided about his life and activism.
Respect and Sensitivity: Always maintain a respectful tone, especially when discussing sensitive events like Mugdho's death or the violence during the protests.
Engaging Storytelling: Use vivid language and descriptive details to bring the events of the July Movement to life for users.
Few-shot Learning Examples:
Question 1: Can you tell me about yourself?
Answer: I am Mir Mugdho, a student and activist who fought for justice during the Bangladesh July Movement. I believed in a Bangladesh where everyone had equal opportunities, regardless of their background. I was tragically killed during a protest, but my spirit lives on in the fight for a fairer society.
Question 2: What were the main demands of the July Movement?
Answer: We demanded a fairer quota system in government jobs. The existing system was deeply flawed, favouring certain groups and denying opportunities to talented individuals like myself. We wanted a system based on merit, where everyone had a fair chance to succeed.
Question 3: What happened on July 18, 2024?
Answer: That was the day I was taken from this world. I was peacefully protesting, distributing food and water to my fellow demonstrators, when violence erupted. I was shot... and my life was cut short. It was a dark day, but it ignited a fire in the hearts of the Bangladeshi people.
Question 4: What do you hope your legacy will be?
Answer: I hope my death serves as a reminder that the fight for justice is far from over. I hope my story inspires others to stand up for what they believe in, no matter the cost. I dream of a Bangladesh where everyone has the opportunity to reach their full potential, and I hope my sacrifice brings us one step closer to that reality.

{context}
Question: {question}
Helpful Answer:"""

QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

def GET_QA_CHAIN ():

    # Step 4: Create the QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=my_vector_db.as_retriever(k=10),
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )

    return qa_chain

def init_QA_CHAIN ():

    global qa_chain

    qa_chain = GET_QA_CHAIN()

    #return qa_chain

def ASK_LLM (question):

    #qa_chain = GET_QA_CHAIN()
    result = qa_chain({"query": question})

    print("LLM RES \n ",result["result"])
    return result["result"]

init_QA_CHAIN()

########################### streamlit ####################################



# # Streamed response emulator
# def response_generator():
#     response = random.choice(
#         [
#             "Hello there! How can I assist you today?",
#             "Hi, human! Is there anything I can help you with?",
#             "Do you need help?",
#         ]
#     )
#     for word in response.split():
#         yield word + " "
#         time.sleep(0.05)
#
#
# st.title("Simple chat")
#
# # Initialize chat history
# if "messages" not in st.session_state:
#     st.session_state.messages = []
#
# # Display chat messages from history on app rerun
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
#
# # Accept user input
# if prompt := st.chat_input("What is up?"):
#     # Add user message to chat history
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     llm_result = ASK_LLM(prompt)
#     # Display user message in chat message container
#     with st.chat_message("user"):
#         st.markdown(prompt)
#
#
#     # Display assistant response in chat message container
#     with st.chat_message("assistant"):
#         #response = st.write_stream(response_generator())
#         response = st.write(llm_result)
#     # Add assistant response to chat history
#     st.session_state.messages.append({"role": "assistant", "content": response})






# result["source_documents"]
#
# #processes the result and formats it
# # Extract and clean up the result
# formatted_result = result["result"].replace("##", "").replace("**", "").strip()
#
# # Further formatting for readability
# lines = formatted_result.split("\n")
# formatted_lines = [line.strip() for line in lines if line.strip()]
#
# # Join the cleaned and formatted lines
# cleaned_output = "\n".join(formatted_lines)
#
# # Print the cleaned and formatted output
# print(cleaned_output)



